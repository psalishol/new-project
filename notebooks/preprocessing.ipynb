{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_col(data_path, chunksize):\n",
    "    for df_iter, chunk in  enumerate(pd.read_csv(data_path, chunksize=chunksize, iterator=False)):\n",
    "        pass\n",
    "    col_list = list(chunk.columns)\n",
    "    return col_list, chunk\n",
    "\n",
    "col_list, data = load_col(r\"..\\Data\\CarSalePrice.csv\", chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING PARAMETER\n",
    "chunksize= 1e5\n",
    "df_dir = r\"..\\Data\\external\\used_cars_data.csv\"\n",
    "# loading chunk of the data for the column list\n",
    "def col_list(data_path, chunksize, progress=None):\n",
    "    print(\"Checking and loading file\")\n",
    "    time.sleep(0.9)\n",
    "    for df_iter, chunk in enumerate(pd.read_csv(data_path, chunksize=chunksize, iterator=False)):\n",
    "        pass\n",
    "    col_list = list(chunk.columns)\n",
    "    return col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Partitioning Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PARTITION = 50    # Number of buckets\n",
    "base_partitions_dir = \"../data/external/Partition\"\n",
    "output_dir = \"../data/external/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Function for Hashing id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashing the listing id to allow even partitioning across the dataset\n",
    "def hash_(listing_id):\n",
    "    \"\"\"Creates an hashed column using the listing id for the vehicle\"\"\"\n",
    "    return int(hashlib.md5(str(listing_id).encode(\"utf-8\")).hexdigest(), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating dir for partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the directory exists...\n",
      "Directory found\n",
      "Removing directory\n",
      "Removed the directory\n",
      "Creating empty folder list for partition\n",
      "| ../data/external/Partition/p0 | Partition left 50 |\n",
      "| ../data/external/Partition/p1 | Partition left 49 |\n",
      "| ../data/external/Partition/p2 | Partition left 48 |\n",
      "| ../data/external/Partition/p3 | Partition left 47 |\n",
      "| ../data/external/Partition/p4 | Partition left 46 |\n",
      "| ../data/external/Partition/p5 | Partition left 45 |\n",
      "| ../data/external/Partition/p6 | Partition left 44 |\n",
      "| ../data/external/Partition/p7 | Partition left 43 |\n",
      "| ../data/external/Partition/p8 | Partition left 42 |\n",
      "| ../data/external/Partition/p9 | Partition left 41 |\n",
      "| ../data/external/Partition/p10 | Partition left 40 |\n",
      "| ../data/external/Partition/p11 | Partition left 39 |\n",
      "| ../data/external/Partition/p12 | Partition left 38 |\n",
      "| ../data/external/Partition/p13 | Partition left 37 |\n",
      "| ../data/external/Partition/p14 | Partition left 36 |\n",
      "| ../data/external/Partition/p15 | Partition left 35 |\n",
      "| ../data/external/Partition/p16 | Partition left 34 |\n",
      "| ../data/external/Partition/p17 | Partition left 33 |\n",
      "| ../data/external/Partition/p18 | Partition left 32 |\n",
      "| ../data/external/Partition/p19 | Partition left 31 |\n",
      "| ../data/external/Partition/p20 | Partition left 30 |\n",
      "| ../data/external/Partition/p21 | Partition left 29 |\n",
      "| ../data/external/Partition/p22 | Partition left 28 |\n",
      "| ../data/external/Partition/p23 | Partition left 27 |\n",
      "| ../data/external/Partition/p24 | Partition left 26 |\n",
      "| ../data/external/Partition/p25 | Partition left 25 |\n",
      "| ../data/external/Partition/p26 | Partition left 24 |\n",
      "| ../data/external/Partition/p27 | Partition left 23 |\n",
      "| ../data/external/Partition/p28 | Partition left 22 |\n",
      "| ../data/external/Partition/p29 | Partition left 21 |\n",
      "| ../data/external/Partition/p30 | Partition left 20 |\n",
      "| ../data/external/Partition/p31 | Partition left 19 |\n",
      "| ../data/external/Partition/p32 | Partition left 18 |\n",
      "| ../data/external/Partition/p33 | Partition left 17 |\n",
      "| ../data/external/Partition/p34 | Partition left 16 |\n",
      "| ../data/external/Partition/p35 | Partition left 15 |\n",
      "| ../data/external/Partition/p36 | Partition left 14 |\n",
      "| ../data/external/Partition/p37 | Partition left 13 |\n",
      "| ../data/external/Partition/p38 | Partition left 12 |\n",
      "| ../data/external/Partition/p39 | Partition left 11 |\n",
      "| ../data/external/Partition/p40 | Partition left 10 |\n",
      "| ../data/external/Partition/p41 | Partition left 9 |\n",
      "| ../data/external/Partition/p42 | Partition left 8 |\n",
      "| ../data/external/Partition/p43 | Partition left 7 |\n",
      "| ../data/external/Partition/p44 | Partition left 6 |\n",
      "| ../data/external/Partition/p45 | Partition left 5 |\n",
      "| ../data/external/Partition/p46 | Partition left 4 |\n",
      "| ../data/external/Partition/p47 | Partition left 3 |\n",
      "| ../data/external/Partition/p48 | Partition left 2 |\n",
      "| ../data/external/Partition/p49 | Partition left 1 |\n",
      "| Completed | Time Taken ------------------------- 36.58788704872131sec |\n"
     ]
    }
   ],
   "source": [
    "def create_partition():\n",
    "    \"\"\"Creates an empty partition directory for the buckets\"\"\"\n",
    "    start = time.time()\n",
    "    print(\"Checking if the directory exists...\")\n",
    "    time.sleep(0.9)\n",
    "    if os.path.exists(base_partitions_dir):\n",
    "        print(\"Directory found\")\n",
    "        time.sleep(0.4)\n",
    "        print(\"Removing directory\")\n",
    "        time.sleep(1)\n",
    "        shutil.rmtree(base_partitions_dir)\n",
    "        print(\"Removed the directory\")\n",
    "    else:\n",
    "        print(\"No Such Directory found.\")\n",
    "\n",
    "    # Delaying before creating the directories\n",
    "    time.sleep(2.5)\n",
    "\n",
    "    print(\"Creating empty folder list for partition\")\n",
    "    time.sleep(0.9)\n",
    "    if not os.path.exists(base_partitions_dir):\n",
    "        # Creating partition directory\n",
    "        os.mkdir(base_partitions_dir)\n",
    "        # Making a new directory for the partitions\n",
    "        for i in range(N_PARTITION):\n",
    "            partition_path = os.path.join(\n",
    "                base_partitions_dir, \"p{}\".format(i)).replace(\"\\\\\", \"/\")\n",
    "            # Printing the path\n",
    "            print('| {} | Partition left {} |'.format(partition_path,N_PARTITION-i))\n",
    "            if not os.path.exists(partition_path):\n",
    "                os.mkdir(partition_path)\n",
    "            else:\n",
    "                print(\"Path Already exist\")\n",
    "            time.sleep(0.6)\n",
    "    end = time.time()\n",
    "    print(\"| Completed | Time Taken ------------------------- {}sec |\".format(str(end-start)))\n",
    "# Making the directory\n",
    "dir = create_partition()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Creating blank partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/external/Partition/p0/\n",
      "../data/external/Partition/p1/\n",
      "../data/external/Partition/p2/\n"
     ]
    }
   ],
   "source": [
    "def create_blank_partition():\n",
    "    \"\"\"Creating a blank partition with the number of bucket\"\"\"\n",
    "    start = time.time()\n",
    "    data_list = col_list(df_dir, chunksize)\n",
    "    for i in range(N_PARTITION):\n",
    "        file_base_dir = os.path.join(base_partitions_dir,\"p{}\".format(str(i)),\"\").replace(\"\\\\\",\"/\")\n",
    "        print(file_base_dir)\n",
    "        # Opening the file and writing it to the partition created\n",
    "        with open(file_base_dir+\"vehicle_used_data.csv\", \"w\") as f:\n",
    "            f.write(\",\".join(data_list))\n",
    "    end = time.time()\n",
    "    print(\"Time taken ------------------- | {}sec\".format(str(end-start)))\n",
    "    return file_base_dir\n",
    "        \n",
    "dir_path = create_blank_partition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning by hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioing and hashing the \n",
    "def partition_by_hashing(df, name , progress= None):\n",
    "    # hashing the listing_id column into the number of partitions\n",
    "    df[\"hashed\"] = df[\"listing_id\"].apply(hash_) % N_PARTITION\n",
    "    for partitions, data in df.groupby(\"hashed\"):\n",
    "        # Wrting the data to the partition\n",
    "        path_dir = os.path.join(base_partitions_dir,\"Vehicle_used_data_{}.csv\".format(str(partitions)))\n",
    "        # Writing the data to the path\n",
    "        with open(path_dir, \"a\") as f:\n",
    "            f.write(\"\\n\")\n",
    "            data.to_csv(f, header=False, index=False)\n",
    "\n",
    "\n",
    "dir = create_blank_partition()\n",
    "os.listdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading file in chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PSALISHOL\\Documents\\My Projects\\Car Prediction\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "chunksize = 1e5\n",
    "temp = pd.read_csv(r\"..\\Data\\used_cars_data.csv\", iterator=True, chunksize=chunksize)\n",
    "df = pd.concat(temp, ignore_index=True)\n",
    "\n",
    "# data = partition_by_hashing(df, name=\"listing_id\", progress=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vin: string (nullable = true)\n",
      " |-- back_legroom: string (nullable = true)\n",
      " |-- bed: string (nullable = true)\n",
      " |-- bed_height: string (nullable = true)\n",
      " |-- bed_length: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- cabin: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_fuel_economy: string (nullable = true)\n",
      " |-- combine_fuel_economy: string (nullable = true)\n",
      " |-- daysonmarket: string (nullable = true)\n",
      " |-- dealer_zip: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- engine_cylinders: string (nullable = true)\n",
      " |-- engine_displacement: string (nullable = true)\n",
      " |-- engine_type: string (nullable = true)\n",
      " |-- exterior_color: string (nullable = true)\n",
      " |-- fleet: string (nullable = true)\n",
      " |-- frame_damaged: string (nullable = true)\n",
      " |-- franchise_dealer: string (nullable = true)\n",
      " |-- franchise_make: string (nullable = true)\n",
      " |-- front_legroom: string (nullable = true)\n",
      " |-- fuel_tank_volume: string (nullable = true)\n",
      " |-- fuel_type: string (nullable = true)\n",
      " |-- has_accidents: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- highway_fuel_economy: string (nullable = true)\n",
      " |-- horsepower: string (nullable = true)\n",
      " |-- interior_color: string (nullable = true)\n",
      " |-- isCab: string (nullable = true)\n",
      " |-- is_certified: string (nullable = true)\n",
      " |-- is_cpo: string (nullable = true)\n",
      " |-- is_new: string (nullable = true)\n",
      " |-- is_oemcpo: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- length: string (nullable = true)\n",
      " |-- listed_date: string (nullable = true)\n",
      " |-- listing_color: string (nullable = true)\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- main_picture_url: string (nullable = true)\n",
      " |-- major_options: string (nullable = true)\n",
      " |-- make_name: string (nullable = true)\n",
      " |-- maximum_seating: string (nullable = true)\n",
      " |-- mileage: string (nullable = true)\n",
      " |-- model_name: string (nullable = true)\n",
      " |-- owner_count: string (nullable = true)\n",
      " |-- power: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- salvage: string (nullable = true)\n",
      " |-- savings_amount: string (nullable = true)\n",
      " |-- seller_rating: string (nullable = true)\n",
      " |-- sp_id: string (nullable = true)\n",
      " |-- sp_name: string (nullable = true)\n",
      " |-- theft_title: string (nullable = true)\n",
      " |-- torque: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- transmission_display: string (nullable = true)\n",
      " |-- trimId: string (nullable = true)\n",
      " |-- trim_name: string (nullable = true)\n",
      " |-- vehicle_damage_category: string (nullable = true)\n",
      " |-- wheel_system: string (nullable = true)\n",
      " |-- wheel_system_display: string (nullable = true)\n",
      " |-- wheelbase: string (nullable = true)\n",
      " |-- width: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Partitioning the data\n",
    "# The dataset contain more than 3million records, \n",
    "# we will partition these records into partition of 50 buckets \n",
    "# making about 60,000 records per bucket\n",
    "\n",
    "\n",
    "# PARTITIONING PARAMETER\n",
    "\n",
    "N_PARTITION = 50    # Number of buckets\n",
    "base_partitions_dir = r\"..\\data\\external\\Partitioned\"\n",
    "output_dir = \"../data/external/output\"\n",
    "\n",
    "# hashing the listing id to allow even partitioning across the dataset\n",
    "def hash_(listing_id):\n",
    "    \"\"\"Creates an hashed column using the listing id for the vehicle\"\"\"\n",
    "    return int(hashlib.md5(str(listing_id).encode(\"utf-8\")).hexdigest(), 16)\n",
    "\n",
    "def create_partition():\n",
    "    \"\"\"Creates an empty partition directory for the buckets\"\"\"\n",
    "    print(\"Checking if the directory exists\")\n",
    "    if os.path.exists(base_partitions_dir):\n",
    "        shutil.rmtree(base_partitions_dir)\n",
    "        print(\"removed the directory\")\n",
    "    else:\n",
    "        print(\"No Such Directory found.\")\n",
    "        \n",
    "    # Delaying before creating the directories\n",
    "    time.sleep(0.5) \n",
    "    \n",
    "    print(\"Creating empty folder list for partition\")\n",
    "    if not os.path.exists(os.path.join(base_partitions_dir,\"root\")):\n",
    "        # Making a new directory for the partitions\n",
    "        for i in range(N_PARTITION):\n",
    "            partition_path = os.path.join(base_partitions_dir, \"root\", i)\n",
    "            print(partition_path)\n",
    "            if not os.path.exists(partition_path):\n",
    "                os.mkdir(partition_path)\n",
    "\n",
    "\n",
    "\n",
    "def create_blank_partition():\n",
    "    \n",
    "    for i in range(N_PARTITION):\n",
    "        \n",
    "        dir = os.path.join(base_partitions_dir, \"root\", i)\n",
    "        file_path = r\"..\\data\\external\\used_cars_data.csv\"\n",
    "        \n",
    "        with open(file_path, \"r\") as data:\n",
    "            with open(dir, \"w\") as f:\n",
    "                f.write(\",\".join(list(data.columns)))\n",
    "     \n",
    "        return dir\n",
    "\n",
    "def partition_by_hashing(df, name , progress= None):\n",
    "    # hashing the listing_id column into the number of partitions\n",
    "    df[\"partition\"] = df[\"listing_id\"].apply(hash_) % N_PARTITION\n",
    "    for partitions, data in df.groupby(\"partition\"):\n",
    "        # Wrting the data to the partition\n",
    "        path_dir =os.path.join(base_partitions_dir,\"root\", partitions)\n",
    "        with open(path_dir, \"w\") as f:\n",
    "            f.write(path_dir, data)\n",
    "        \n",
    "dir = create_blank_partition()\n",
    "os.listdir(dir)\n",
    "\n",
    "\n",
    "# Making a sparksession\n",
    "SPARK_SESSION = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Preprocessing with Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading the data\n",
    "df = SPARK_SESSION.read.csv(\n",
    "    r\"..\\data\\external\\used_cars_data.csv\", header=True, inferSchema=True )\n",
    "\n",
    "#using the main file for the above and that is the only thing for now\n",
    "\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "# Using the current apache spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making list of the variable to use for the dataframe\n",
    "cols = [\"region\",\"price\",\"year\",\"manufacturer\",\"model\",\n",
    "            \"condition\",\"cylinders\",\"fuel\",\"odometer\",\"transmission\",\n",
    "                \"drive\",\"size\",\"type\",\"state\",\"lat\",\"long\",\"posting_date\"]\n",
    "# Reading the file\n",
    "data_f = pd.read_csv(\n",
    "    r\"..\\data\\external\\vehicles.csv\", sep=\",\", usecols=cols)\n",
    "\n",
    "# Making a copy of the data\n",
    "data = data_f.copy()\n",
    "# converting the year posted to pandas datetime format\n",
    "data['posting_date'] = pd.to_datetime(data['posting_date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region               0\n",
       "price                0\n",
       "year              1205\n",
       "manufacturer     17646\n",
       "model             5277\n",
       "condition       174104\n",
       "cylinders       177678\n",
       "fuel              3013\n",
       "odometer          4400\n",
       "transmission      2556\n",
       "drive           130567\n",
       "size            306361\n",
       "type             92858\n",
       "state                0\n",
       "lat               6549\n",
       "long              6549\n",
       "posting_date        68\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped size\n"
     ]
    }
   ],
   "source": [
    "def del_var(dataset):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        dataset ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    for feature in dataset.columns:\n",
    "        if dataset[feature].isnull().mean() > 0.5:\n",
    "            dataset = dataset.drop(feature, axis=1)\n",
    "            print(\"dropped {}\".format(feature))\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "data_ = del_var(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>condition</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>fuel</th>\n",
       "      <th>odometer</th>\n",
       "      <th>transmission</th>\n",
       "      <th>drive</th>\n",
       "      <th>type</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>posting_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prescott</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>az</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fayetteville</td>\n",
       "      <td>11900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>florida keys</td>\n",
       "      <td>21000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worcester / central MA</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>greensboro</td>\n",
       "      <td>4900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   region  price  year manufacturer model condition cylinders  \\\n",
       "0                prescott   6000   NaN          NaN   NaN       NaN       NaN   \n",
       "1            fayetteville  11900   NaN          NaN   NaN       NaN       NaN   \n",
       "2            florida keys  21000   NaN          NaN   NaN       NaN       NaN   \n",
       "3  worcester / central MA   1500   NaN          NaN   NaN       NaN       NaN   \n",
       "4              greensboro   4900   NaN          NaN   NaN       NaN       NaN   \n",
       "\n",
       "  fuel  odometer transmission drive type state  lat  long posting_date  \n",
       "0  NaN       NaN          NaN   NaN  NaN    az  NaN   NaN          NaN  \n",
       "1  NaN       NaN          NaN   NaN  NaN    ar  NaN   NaN          NaN  \n",
       "2  NaN       NaN          NaN   NaN  NaN    fl  NaN   NaN          NaN  \n",
       "3  NaN       NaN          NaN   NaN  NaN    ma  NaN   NaN          NaN  \n",
       "4  NaN       NaN          NaN   NaN  NaN    nc  NaN   NaN          NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(type='van'),\n",
       " Row(type='mini-van'),\n",
       " Row(type='offroad'),\n",
       " Row(type='wagon'),\n",
       " Row(type=None),\n",
       " Row(type='coupe'),\n",
       " Row(type='bus'),\n",
       " Row(type='SUV'),\n",
       " Row(type='other'),\n",
       " Row(type='convertible'),\n",
       " Row(type='-121.7473'),\n",
       " Row(type='sedan'),\n",
       " Row(type='hatchback'),\n",
       " Row(type='truck'),\n",
       " Row(type='pickup'),\n",
       " Row(type=' used cars'),\n",
       " Row(type=' 645'),\n",
       " Row(type=' accuracy'),\n",
       " Row(type=' GMC '),\n",
       " Row(type=' Orlando Car Deals'),\n",
       " Row(type=' dually'),\n",
       " Row(type=' S550')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"type\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the normal operation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "227f792da0f6ad2096ba6827abad12bec8204d048867e99faf812469bc2d14d6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
